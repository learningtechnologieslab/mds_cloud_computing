{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21504fa-388f-4f79-8424-3d0bf3757e93",
   "metadata": {},
   "source": [
    "# Introduction to MapReduce\n",
    "\n",
    "MapReduce is a programming model designed for processing and generating large datasets in a distributed and parallel manner. It was popularized by Google to handle the immense volume of data processed in their infrastructure. At its core, MapReduce simplifies data processing by breaking it into two main phases:\n",
    "\n",
    "#### Map Phase:\n",
    "In this phase, the input data is divided into smaller chunks. A function called the mapper processes each chunk and transforms it into intermediate key-value pairs. For example, in a word count task, the mapper takes lines of text and outputs each word as a key paired with the value 1.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "* Input: [\"Hello world\", \"Hello MapReduce\"]\n",
    "* Output: [(\"hello\", 1), (\"world\", 1), (\"hello\", 1), (\"mapreduce\", 1)]\n",
    "\n",
    "#### Reduce Phase:\n",
    "The intermediate key-value pairs are then grouped by key (word in our example). A function called the reducer processes each group to aggregate the values, often by summing, counting, or performing another operation.\n",
    "\n",
    "**Example**:\n",
    "\n",
    "* Input: [(\"hello\", [1, 1]), (\"world\", [1]), (\"mapreduce\", [1])]\n",
    "* Output: [(\"hello\", 2), (\"world\", 1), (\"mapreduce\", 1)]\n",
    "\n",
    "#### Why MapReduce?\n",
    "* **Scalability**: It can process vast amounts of data by distributing tasks across multiple machines.\n",
    "* **Simplicity**: The user only needs to write two functions: mapper and reducer.\n",
    "* **Fault Tolerance**: It automatically handles machine failures by redistributing tasks to other machines.\n",
    "* **Parallelism**: Each chunk of data is processed independently, allowing for efficient parallel processing.\n",
    "* **Real-World Applications**: MapReduce is commonly used for:\n",
    "    * Word count in large text corpora\n",
    "    * Log analysis\n",
    "    * Indexing web pages\n",
    "    * Analyzing large datasets (e.g., clickstream data, social network data)\n",
    "      \n",
    "This model forms the foundation of many modern distributed systems like Hadoop and Spark, making it a crucial concept in big data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469f9f0-b14b-47bf-b0c2-eaa6810535d6",
   "metadata": {},
   "source": [
    "### Import Necessary Libraries\n",
    "\n",
    "We will use Python's built-in collections library to help with grouping data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bff9de1-1f70-4df2-80b5-aa2e2da64a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e70878-7dd1-4188-83a5-d3a14b74d766",
   "metadata": {},
   "source": [
    "### Sample Data\n",
    "\n",
    "Let's define some sample text data to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d664550-39f8-44cf-9bd0-df61ededd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data: List of documents (strings)\n",
    "documents = [\n",
    "    \"Hello world\",\n",
    "    \"Hello MapReduce\",\n",
    "    \"Hello world of MapReduce\",\n",
    "    \"Big data analytics\",\n",
    "    \"Data science and big data\",\n",
    "    \"Analytics in the world of big data\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac33726-cf83-4345-a48b-8f32a8821f0f",
   "metadata": {},
   "source": [
    "### Map Function\n",
    "\n",
    "In the MapReduce model, the Map function processes input data and produces intermediate key/value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3ba89ea-3ac5-4cad-a54e-57ce58d37bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map function\n",
    "def mapper(document):\n",
    "    \"\"\"\n",
    "    The mapper function takes a document (string) and yields (word, 1) pairs.\n",
    "    \"\"\"\n",
    "    for word in document.split():\n",
    "        yield (word.lower(), 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f206ca-1994-4c8e-b2c5-b12835763266",
   "metadata": {},
   "source": [
    "### Reduce Function\n",
    "\n",
    "The Reduce function merges all intermediate values associated with the same key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b79a6f-e10f-4538-872b-de5e4704955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce function\n",
    "def reducer(word, counts):\n",
    "    \"\"\"\n",
    "    The reducer function takes a word and a list of counts, and returns (word, total_count).\n",
    "    \"\"\"\n",
    "    return (word, sum(counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476485cb-0714-43a6-b36b-eb0f06b42f7b",
   "metadata": {},
   "source": [
    "### Map Phase\n",
    "\n",
    "We will apply the mapper function to each document to generate intermediate key/value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37372e08-aab3-4a2b-93c4-a3cad6ed6c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map phase\n",
    "mapped = []  # List to store mapped data\n",
    "for document in documents:\n",
    "    # Apply the mapper to each document and collect the results\n",
    "    mapped.extend(list(mapper(document)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea317364-71e8-4e14-846d-d6cfbdfeff20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped data:\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "('hello', 1)\n",
      "('mapreduce', 1)\n",
      "('hello', 1)\n",
      "('world', 1)\n",
      "('of', 1)\n",
      "('mapreduce', 1)\n",
      "('big', 1)\n",
      "('data', 1)\n",
      "('analytics', 1)\n",
      "('data', 1)\n",
      "('science', 1)\n",
      "('and', 1)\n",
      "('big', 1)\n",
      "('data', 1)\n",
      "('analytics', 1)\n",
      "('in', 1)\n",
      "('the', 1)\n",
      "('world', 1)\n",
      "('of', 1)\n",
      "('big', 1)\n",
      "('data', 1)\n"
     ]
    }
   ],
   "source": [
    "# Print mapped data\n",
    "\n",
    "print(\"Mapped data:\")\n",
    "for item in mapped:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c1a39-cb2d-4868-bba0-728570507689",
   "metadata": {},
   "source": [
    "### Shuffle and Sort Phase\n",
    "\n",
    "In MapReduce, after the Map phase, the framework sorts and groups the data by key (word). We can simulate this using a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "555d22e6-9461-4702-abed-cf0a7127b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and sort phase\n",
    "grouped = defaultdict(list)\n",
    "for word, count in mapped:\n",
    "    # Group counts by word\n",
    "    grouped[word].append(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6245af7-a84f-43c6-a0ce-c16fa998296b",
   "metadata": {},
   "source": [
    "### Grouped Data\n",
    "\n",
    "Let's see the data after grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee03f265-4145-47b8-ab5a-a2e1bf1e7f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouped data:\n",
      "hello: [1, 1, 1]\n",
      "world: [1, 1, 1]\n",
      "mapreduce: [1, 1]\n",
      "of: [1, 1]\n",
      "big: [1, 1, 1]\n",
      "data: [1, 1, 1, 1]\n",
      "analytics: [1, 1]\n",
      "science: [1]\n",
      "and: [1]\n",
      "in: [1]\n",
      "the: [1]\n"
     ]
    }
   ],
   "source": [
    "# Print grouped data\n",
    "print(\"\\nGrouped data:\")\n",
    "for word, counts in grouped.items():\n",
    "    print(f\"{word}: {counts}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64cb54-a84d-498b-b1b5-b53c3ac7964c",
   "metadata": {},
   "source": [
    "### Reduce Phase\n",
    "\n",
    "We will apply the reducer function to each group to aggregate the counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e19d560f-9f22-4799-b929-2616f25867e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce phase\n",
    "reduced = []  # List to store reduced data\n",
    "for word, counts in grouped.items():\n",
    "    # Apply the reducer to each word and its list of counts\n",
    "    reduced.append(reducer(word, counts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc7527-3304-4bb2-988d-dbeef86eab9c",
   "metadata": {},
   "source": [
    "### Reduced Data\n",
    "\n",
    "Let's see the data after the Reduce phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eede7818-6c15-4336-883b-d62f41cfa3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced data:\n",
      "hello: 3\n",
      "world: 3\n",
      "mapreduce: 2\n",
      "of: 2\n",
      "big: 3\n",
      "data: 4\n",
      "analytics: 2\n",
      "science: 1\n",
      "and: 1\n",
      "in: 1\n",
      "the: 1\n"
     ]
    }
   ],
   "source": [
    "# Print reduced data\n",
    "print(\"\\nReduced data:\")\n",
    "for word, total in reduced:\n",
    "    print(f\"{word}: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7ceb8-98e8-4bf8-8ccb-55d31a916e98",
   "metadata": {},
   "source": [
    "### Final Result\n",
    "\n",
    "Let's sort and present the final word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63be8fd9-ca4e-4932-b6eb-c10b391334a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final word counts:\n",
      "analytics: 2\n",
      "and: 1\n",
      "big: 3\n",
      "data: 4\n",
      "hello: 3\n",
      "in: 1\n",
      "mapreduce: 2\n",
      "of: 2\n",
      "science: 1\n",
      "the: 1\n",
      "world: 3\n"
     ]
    }
   ],
   "source": [
    "# Final result\n",
    "print(\"\\nFinal word counts:\")\n",
    "# Sort the results by word\n",
    "result = sorted(reduced, key=lambda x: x[0])\n",
    "for word, total in result:\n",
    "    print(f\"{word}: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260c3dbb-d1b3-43c3-9e25-85ed1b86af68",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have successfully implemented a simple MapReduce example in Python to count word frequencies in a set of documents. This example illustrates the MapReduce flow:\n",
    "\n",
    "* **Map**: Process input data and produce intermediate key/value pairs.\n",
    "* **Shuffle and Sort**: Group intermediate data by key.\n",
    "* **Reduce**: Aggregate the grouped data to produce the final result.\n",
    "  \n",
    "This approach can be scaled up using distributed computing frameworks like Hadoop or Spark for processing large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f99542-e391-4129-94df-c13ac8b7ee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
